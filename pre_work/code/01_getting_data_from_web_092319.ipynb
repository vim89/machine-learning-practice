{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data from the Web\n",
    "\n",
    "_Authors_: Riebeeck van Niekerk & Jon-Cody Sokoll\n",
    "\n",
    "### Workshop Duration: 10:40AM - 12:00PM\n",
    "\n",
    "*80 Minutes*\n",
    "\n",
    "An introduction to programmatically accessing data from websites and APIs using Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this Notebook <a id=\"1\"></a>\n",
    "\n",
    "#### 1. Examples: This is your opportunity to observe and learn. Most of the examples contains code and information that could be used to solve you exercise.\n",
    "#### 2. Exercises: This is your opportunity to get hands on and try solving the challenges.\n",
    " \n",
    " - *Running the cells in the example section may break since there are references to file paths not on your local system.*\n",
    " - *If you run a cell that references a library that has not bee install you will see an error. Work with the supporting instructors to get this set up if you are having trouble.*\n",
    " - *This workshop contains lots of infomration and time is limited. We encourage everyone to spend time working these examples and exercises in detail after completing this bootcamp.*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AGENDA\n",
    "\n",
    "- Example 1: Getting Movie Data from IMDB (10 Minutes)\n",
    "- Exercise 1: Movie Release Date: defining a function (5 Minutes)\n",
    "- Example 2: Color Wheel (10 Minutes)\n",
    "- Example 3: Plotly - Brief Overview (5 Minutes)\n",
    "- Exercise 2: Getting Data Using API Calls (30 Minutes)\n",
    "- Example 4: Intro to Web Scraping (10 Minutes)\n",
    "- Example 5: Weather Analysis (5 Minutes - SKIP TO UFO's)\n",
    "- Exercise 4: UFO Sigtings (10-15 Minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API (Application Programming Interface) <a id=\"1\"></a>\n",
    "\n",
    "What is an API?\n",
    "- Structured way to expose specific functionality and data to users\n",
    "- Web APIs usually follow the [REST](https://en.wikipedia.org/wiki/Representational_state_transfer) standard (i.e. stateless)\n",
    "\n",
    "How to interact with an API:\n",
    "- Make a \"request\" to a specific URL (an \"endpoint\"), and get the data back in a \"response\"\n",
    "- Most relevant request method for us is GET (other methods: POST, PUT, DELETE)\n",
    "- Response is often JSON or XML format\n",
    "- Web console is sometimes available (allows you to explore an API)\n",
    "\n",
    "## Supplementary Reading\n",
    "\n",
    "1. [Requests: Python Library Documentation](http://docs.python-requests.org/en/master/user/quickstart/)\n",
    "\n",
    "2. [OAuth2 Documentation](https://oauth.net/2/)\n",
    "\n",
    "3. [What is an API & How Does is Work](https://blogs.mulesoft.com/biz/tech-ramblings-biz/what-are-apis-how-do-apis-work/)\n",
    "\n",
    "4. [API Directory](https://www.programmableweb.com/apis/directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Getting Movie Data from IMDB <a id=\"1B\"></a>\n",
    "*10 Minutes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(requests.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Requests Library\n",
    "We will submit a get request to specific movie to the URL: `http://www.omdbapi.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"53bfc95d\" # <- Super Secret Shhhh\n",
    "title = \"Jurassic Park\" #Search for a movie you like\n",
    "url = 'http://www.omdbapi.com?'\n",
    "\n",
    "payload = {'t': title,\n",
    "           'apikey': API_KEY}\n",
    "\n",
    "r = requests.get('http://www.omdbapi.com?', params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the status: 200 means success, 4xx or 5xx means error\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know from the documentation on omdapi.com that the response is a `JSON` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can call out specific elements\n",
    "r.json()['Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if the movie isn't found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {'apikey': API_KEY, \n",
    "          't': 'Machine Learning Rules!'}\n",
    "\n",
    "r = requests.get(url, params=payload)\n",
    "r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 Movie Release Date\n",
    "*5 Minutes*\n",
    "##### Define a function to return the year of release of a given movie title, return None if no movie found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movie_year_from_title(movie_title):\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_movie_year_from_title(\"Jungle Book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Use Case Discussion\n",
    "* How might our clients ask us to leverage text data such as this to inform business decisions?\n",
    "* What could we do if we mashed this movie meta data up with movie review data from a source such as Rotten Tomatoes? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Color Wheel <a id='1C'></a>\n",
    "*10 Minutes*\n",
    "\n",
    "In this example, accessing data on some of the colors of CrayolaÂ® palette via the Smithosian Cooper Hewitt's API. Archievists at Cooper Hewitt use this palette to tag images of objects by color. Downstream, these tags allow for a greater accuracy in information retrival for users looking for objects of a certain hue.\n",
    "\n",
    "### Business Use Case\n",
    "A major online retailer aggregator has asked to Deloitte to help it increase the efficetiveness of its search engine. They found that users are searching their catalog of over 100,000 items by color. However, many of the items don't have color tags making the search process frustrating for users. \n",
    "\n",
    "Our role will be to leverage the Cooper Hewitt's extensive online catalog of design objects tagged with colors to build a training set for automated tagging for the client with machine learning. \n",
    "\n",
    "If our future algorithm is successful, it will save our client thousands of hours to manual labor tagging the images on their website.\n",
    "\n",
    "\n",
    "*__Note:__ This API implements the new standard for API autentication by using OAuth2 with access tokens. I've created a token for us ahead of time.*\n",
    "\n",
    "[Article on Cooper Hewitt's API](https://labs.cooperhewitt.org/2014/the-api-at-the-center-of-the-museum/)\n",
    "\n",
    "[API Documentation](https://collection.cooperhewitt.org/api/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"84976de03204c1d366ae0224bf21d103\" # less secure\n",
    "token = \"2f49c9d05b2faf779d11420637d99f57\" # more secure which uses the OAuth2 authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://api.collection.cooperhewitt.org/rest/?method=cooperhewitt.colors.palettes.getInfo&access_token=%s&palette=crayola' % token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_hex = list(r.json()['colors'].keys())\n",
    "names = [k['name'] for k in list(r.json()['colors'].values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame of the results\n",
    "crayola= pd.DataFrame({'hex': _hex, 'name':names})\n",
    "\n",
    "# crayola.describe()\n",
    "crayola.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Red/Blue/Green\n",
    "The hex code is by design very dense information. Let's parse out the individual color components from the data.\n",
    "\n",
    "[Hex to RGB Converstion by Hand](https://www.rapidtables.com/convert/color/how-hex-to-rgb.html)\n",
    "\n",
    "[Hex to RGB Code](https://stackoverflow.com/questions/29643352/converting-hex-to-rgb-value-in-python)\n",
    "\n",
    "[int() Class](https://docs.python.org/3.4/library/functions.html?highlight=int#int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# function that converts a single list element into it's corresponding RGB Value\n",
    "\n",
    "def hex_to_rbg(_hexcode):\n",
    "    h = _hexcode.lstrip('#') # strips the function of the hash \n",
    "    rbg = tuple(int(h[i:i+2], 16) for i in (0, 2 ,4)) #int converts our hex to a rgb value for us by passing base = 16\n",
    "    return rbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rbg = [hex_to_rbg(h) for h in crayola['hex'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbg = pd.DataFrame(rbg, columns=['red', 'green', 'blue'])\n",
    "crayola = pd.concat([crayola, rbg], axis=1)\n",
    "crayola.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crayola.to_csv('/Users/rvanniekerk/OneDrive - Deloitte (O365D)/ML Guild Emminence/Apprentice ML Guild Course/Apprentice_Level_04222019/Day 1/Outputs/crayola.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3:  Plotly - Brief Overview\n",
    "*5 Minutes*\n",
    "\n",
    "Python plotting library for collaborative, interactive, publication-quality graphs.\n",
    "\n",
    "[Plotly Website Link](https://pypi.org/project/plotly/)\n",
    "\n",
    "*Note: Plotly will not work if you have run the above cells multiple times. Please click on 'Kernel' and select Restart & Clear Output as the incoming data needs to be correct*\n",
    "*Note: Plotly also has a rendering bug while using JupyterLab, so if your graph does not render open a Jupyter Notebook as that should work*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import plotly.offline as py\n",
    "# import plotly.graph_objs as go\n",
    "# py.init_notebook_mode(connected=False)\n",
    "\n",
    "import plotly.offline as py\n",
    "from plotly import __version__\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "from plotly.graph_objs import Scatter\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "py.init_notebook_mode(connected=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Plotly Version\",__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter3d(\n",
    "    x = crayola['red'],\n",
    "    y = crayola['green'],\n",
    "    z = crayola['blue'],\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        color = crayola['hex'].tolist(),\n",
    "        size = 5,\n",
    "        symbol = 'circle',\n",
    "        opacity = 1))\n",
    "\n",
    "layout = go.Layout(margin=dict(l=0, r=0, b=0, t=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = go.Figure(data=[trace],layout=layout)\n",
    "py.iplot(fig, filename='Crayola_Scatter.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise # 2: Getting Data\n",
    "*30 Minutes*\n",
    "\n",
    "Build out our training dataset by studying the API documentation on the Cooper Hewit Website. We need a dataset with the museum curent objects on display (only 100 items), the images associated with those items, and the color(s) of those items.\n",
    "\n",
    "* Store the the name of the objects and other meta data in a csv called `current_collection.csv`\n",
    "* Place the images in a folder named `collection_images`.\n",
    "  * You can used the request method `content` to access the file to write it to a file.\n",
    "  * Raw Example: `open('image.jpg', 'wb').write(request.content)`\n",
    "* Grab the color information and place it another csv `current_collection_colors.csv`\n",
    "\n",
    "The API documentation is available [here](https://collection.cooperhewitt.org/api/methods/). You will want to use the following end points: \n",
    "1. [`getOnDisplay`](https://collection.cooperhewitt.org/api/methods/cooperhewitt.objects.getOnDisplay)\n",
    "2. [`getImages`](https://collection.cooperhewitt.org/api/methods/cooperhewitt.objects.getImages)\n",
    "3. [`getColors`](https://collection.cooperhewitt.org/api/methods/cooperhewitt.objects.getColors)\n",
    "\n",
    "key = \"84976de03204c1d366ae0224bf21d103\", \n",
    "token = \"2f49c9d05b2faf779d11420637d99f57\"\n",
    "\n",
    "[hint 1: Break down your problem and use pd.DataFrame.from_records(r.json()) or just simply r.json() to view data as a starting point](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.from_records.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex: 2.1 - (getOnDisplay) Let's start by using the object.getOnDisplay api call to get all items on display at the Museum \n",
    "\n",
    "1. You're free to achieve this however you wish.\n",
    "2. Limit the number of items in your DataFrame to 100.\n",
    "3. Write all 100 resultant rows and fields to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here's something to get you started, it contains the URL and payload with embedded method you need to use.\n",
    "token = \"2f49c9d05b2faf779d11420637d99f57\"\n",
    "api = 'https://api.collection.cooperhewitt.org/rest/?'\n",
    "\n",
    "\n",
    "payload = {'access_token': token,\n",
    "              'method': 'cooperhewitt.objects.getOnDisplay'}\n",
    "\n",
    "r = requests.get(api, params=payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by breaking down the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you understand how your data is structured and what you would like to achieve, try building a function to do this for you. For this excercise it's not a requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your results to a CSV using the to_csv() method - you can quickly determine a path by typing \"pwd\" into a cell and running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex: 2.2 - getImages: Now let's build a function that uses the object.getImages method api call to retrieve and store our images.\n",
    "\n",
    "1. Grab 100 Images using the getImages method.\n",
    "2. Write all 100 image's to the specified folder on your local system (see hints below).\n",
    "3. Remember each image has an object_id associated with it incase you were wondering which variable to loop through.\n",
    "\n",
    "[hint 2: os python library helps you write/read files on your local system](https://docs.python.org/3/library/os.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's something to get you started, it contains the URL and payload with embedded method you need to use. Please note that in this case a function would \n",
    "# be very helpfull in order to loop through your data. I've given you a head start by passing a single object_id. You will need to figure out how to loop through your set of 100 ojbect\n",
    "# id's, writing them to the folder called collection_images.\n",
    "\n",
    "token = \"2f49c9d05b2faf779d11420637d99f57\"\n",
    "api = 'https://api.collection.cooperhewitt.org/rest/?'     \n",
    "payload = {'access_token': token,\n",
    "           'method': 'cooperhewitt.objects.getImages',\n",
    "           'object_id': '18488027'}\n",
    "\n",
    "r = requests.get(api, params=payload)\n",
    "\n",
    "obj = pd.DataFrame.from_records(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by breaking down the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you understand how your data is structured and what you would like to achieve, you'll need to build a function that loops through your objects to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function sending a request to the url/token using the API method getImages and write each image to the specified file path .../your_directory/collection_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex: 2.3 - getColors: Now let's build a function that uses the object.getColors method api call to retrieve and store the 'colors' in a .csv file\n",
    "\n",
    "1. You will want to do something similar to the previous two exercises, except this time you will be writing the data retreived to a csv.\n",
    "2. Done worry about parsing out the 'colors' field or converting the hex values. Just grab the data and drop it into a .csv.\n",
    "3. Limit the data to 100 rows as in the previous two exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heres' something to get you started. We're going to give you less information this time around since if you've made it this far you're doing well.\n",
    "\n",
    "payload = {'access_token': token,\n",
    "           'method': 'cooperhewitt.objects.getColors',\n",
    "           'object_id': i}\n",
    "\n",
    "r = requests.get(api, params=payload)\n",
    "\n",
    "obj_col = pd.DataFrame.from_records(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by breaking down the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you understand how your data is structured and what you would like to achieve, you would probably smart to build a function that loops through your objects to accomplish this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run your function sending a request to the url/token using the API method getImages and write each image to the specified file path .../your_directory/current_collection_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Intro to Web Scraping <a id=2></a>\n",
    "\n",
    "*5 - 10 Minutes*\n",
    "\n",
    "Often times data is not available in the neat & tidy formats we are used from databases and APIs. We need to go out into the world and capture the data.\n",
    "\n",
    "Enter web scraping which is the process of crawling a website(s) and extracting structured information from the pages of the site(s).\n",
    "\n",
    "There are a whole host of ethical concerns with web scraping. Make sure to read a site's `robots.txt` before initating a web scraping project: ex. https://www.buzzfeed.com/robots.txt\n",
    "\n",
    " - [Beautiful Soup: Python bs4 lib](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    " - [Regexp: Python re lib](https://docs.python.org/3/library/re.html)\n",
    "\n",
    " - [HTML Tags](https://www.w3schools.com/tags/tag_p.asp)\n",
    "\n",
    " - [What is a robot.txt file and how to find it](https://moz.com/learn/seo/robotstxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # a python HTML parser\n",
    "import re #Regular expressions\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data <a id=2A></a>\n",
    "\n",
    "Let's focus on grabbing general weather data & forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://forecast.weather.gov/MapClick.php?lat=38.8904&lon=-77.032#.WpNL-ejwaUk\"\n",
    "r = requests.get(url)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's make some soup\n",
    "soup = BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_day = soup.find(id=\"seven-day-forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seven_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_items = seven_day.find_all(class_=\"tombstone-container\")\n",
    "forecast_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tonight = forecast_items[0]\n",
    "print(tonight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tonight.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting information from the page\n",
    "\n",
    "As you can see, inside the forecast item tonight is all the information we want. There are 4 pieces of information we can extract:\n",
    "\n",
    "* The name of the forecast item â in this case, Tonight.\n",
    "* The description of the conditions â this is stored in the title property of img.\n",
    "* A short description of the conditions.\n",
    "* The temperature low.\n",
    "\n",
    "We'll extract the name of the forecast item, the short description, and the temperature first, since they're all similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = tonight.find(class_=\"period-name\").get_text()\n",
    "short_desc = tonight.find(class_=\"short-desc\").get_text()\n",
    "temp = tonight.find(class_=\"temp\").get_text()\n",
    "\n",
    "print(period)\n",
    "print(short_desc)\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tonight.find(\"img\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can extract the `title` attribute from the `img` tag. To do this, we just treat the BeautifulSoup object like a dictionary, and pass in the attribute we want as a key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tonight.find(\"img\")\n",
    "desc = img['title']\n",
    "\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extracting all the information from the page\n",
    "Now that we know how to extract each individual piece of information, we can combine our knowledge with css selectors and list comprehensions to extract everything at once.\n",
    "\n",
    "In the below code, we:\n",
    "\n",
    "* Select all items with the class `period-name` inside an item with the class `tombstone-container` in `seven_day`.\n",
    "* Use a list comprehension to call the `get_text` method on each `BeautifulSoup` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a reminder here is what we are working with\n",
    "print(tonight.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period_tags = seven_day.select(\".tombstone-container .period-name\")\n",
    "periods = [pt.get_text() for pt in period_tags]\n",
    "periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#breaking it down\n",
    "period_tags = seven_day.select(\".tombstone-container .period-name\")\n",
    "period_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [pt.get_text() for pt in period_tags]\n",
    "periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_descs = [sd.get_text() for sd in seven_day.select(\".tombstone-container .short-desc\")]\n",
    "temps = [t.get_text() for t in seven_day.select(\".tombstone-container .temp\")]\n",
    "descs = [d[\"title\"] for d in seven_day.select(\".tombstone-container img\")]\n",
    "\n",
    "print(short_descs)\n",
    "print(temps)\n",
    "print(descs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_desc = [sd.get_text() for sd in seven_day.select(\".tombstone-container .short-desc\")]\n",
    "print(short_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temps = [t.get_text() for t in seven_day.select(\".tombstone-container .temp\")]\n",
    "print(temps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = [d[\"title\"] for d in seven_day.select(\".tombstone-container img\")]\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Weather Analysis\n",
    "\n",
    "#### **(Skip to UFO's Exercise if time is short)**\n",
    "\n",
    "Combine all the newly scraped data and analyze it. In order to do this, we'll call the DataFrame class, and pass in each list of items that we have. We pass them in as part of a dictionary. Each dictionary key will become a column in the DataFrame, and each list will become the values in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_desc = pd.DataFrame(data=desc,columns=['desc'])\n",
    "frame_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_periods = pd.DataFrame(data=periods,columns=['periods'])\n",
    "frame_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_short_descs = pd.DataFrame(data=short_desc,columns=['short_desc'])\n",
    "frame_short_descs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_temps = pd.DataFrame(data=temps,columns=['temps'],)\n",
    "frame_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = pd.concat([frame_desc, frame_periods, frame_short_descs, frame_temps], axis=1)\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Weather\n",
    "\n",
    "Here you will need to have some kind of understanding of regexp pattern syntax. As always if you don't our friend google is here to assist\n",
    "\n",
    "[REGEX CHEAT SHEET](https://www.dataquest.io/blog/regex-cheatsheet/): Refresher\n",
    "\n",
    "[PYTHON REGEX DOCS](https://docs.python.org/3/library/re.html): PyDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we will extract the number from the temps columns so we can run some basic functions\n",
    "\n",
    "temp_nums = weather[\"temps\"].str.extract(\"(?P<temp_num>\\d+)\", expand=False)\n",
    "temp_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we will simply add in the temp_num column to our dataframe ensuring to cast is as dtype int so we can run some calcs\n",
    "\n",
    "weather[\"temp_num\"] = temp_nums.astype('int')\n",
    "weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather[\"temp_num\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Night Time Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_night = weather[\"temps\"].str.contains(\"Low\")\n",
    "weather[\"is_night\"] = is_night\n",
    "is_night\n",
    "weather[is_night]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXERCISE 4: UFO Sigtings\n",
    "\n",
    "### *with the remaining time, please attempt attempt the exercise below*\n",
    "\n",
    "1. Use beautiful soup to inspect the html file associated with the http://www.nuforc.org/webreports/ndxe201608.html data.\n",
    "2. Use the findAll() and findChildren methods() to loop through the html data and load it into a DataFrame.\n",
    "\n",
    "*HINT you may want to consider using an embedded for loop using the two find methods above; If you can find another more elegant way to do it please share withe the group*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # a python HTML parser\n",
    "import re #Regular expressions\n",
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"http://www.nuforc.org/webreports/ndxe201608.html\")\n",
    "b = BeautifulSoup(r.text, 'html.parser')\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r.content #go ahead and uncomment this line by hitting ctrl+/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b #go ahead and uncomment this as well and compare the different... then bask in the glory of BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What data do we have? Let's look at the head of the HTML file to determine what's contained within.\n",
    "d = b.findAll('thead')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the first sighting\n",
    "for tr in b.findAll('tr', attrs = {'valign':'TOP'})[:1]: # remove the '1' in the slice to view all data\n",
    "    # the findChildren method returns all children underneath it\n",
    "    for child in tr.findChildren():\n",
    "        print(child.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK, it's a bit messy, Let's clean it up. Go ahead and use the code below completing the loops below to load the data into a DataFrame.\n",
    "# Looks like the first element is the date, the 4th is the city, 6th if state, 8th is shape etc...\n",
    "\n",
    "\n",
    "ufo_sightings = {\n",
    "        'Date':[],\n",
    "        'City':[],\n",
    "        'State':[],\n",
    "        'Shape':[],\n",
    "        'Duration':[],\n",
    "        'Summary':[]\n",
    "    }\n",
    "\n",
    "for tr in b.findAll('tr', attrs = {'valign':'TOP'}):\n",
    "        #your code goes here\n",
    "    for child in tr.findChildren():\n",
    "        #your code goes here\n",
    "        \n",
    "    pd.DataFrame(ufo_sightings).head()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "0.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
