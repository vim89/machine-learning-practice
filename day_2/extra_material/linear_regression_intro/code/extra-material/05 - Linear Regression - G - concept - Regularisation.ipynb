{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning_files/Image%20[12].png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simple terms, regularization is tuning or selecting the preferred level of model complexity so your models are better at predicting (generalizing). If you don't do this your models may be too complex and overfit or too simple and underfit, either way giving poor predictions.\n",
    "\n",
    "If you least-squares fit a complex model to a small set of training data you will probably overfit, this is the most common situation. The optimal complexity of the model depends on the sort of process you are modeling and the quality of the data, so there is no a-priori _correct_ complexity of a model.\n",
    "\n",
    "To regularize you need 2 things:\n",
    "\n",
    "1. A way of testing how good your models are at prediction, for example using cross-validation or a set of validation data (you can't use the _fitting error_ for this).\n",
    "2. A tuning parameter which lets you change the complexity or smoothness of the model, or a selection of models of differing complexity/smoothness.\n",
    "\n",
    "Basically you adjust the complexity parameter (or change the model) and find the value which gives the best model predictions.\n",
    "\n",
    "Note that the optimized regularization error will not be an accurate estimate of the overall prediction error so after regularization you will finally have to use an additional validation dataset or perform some additional statistical analysis to get an unbiased prediction error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ModelComplexity in OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One method of understanding 'model complexity' is to define **complexity** as a function of the size of the coefficients.\n",
    "\n",
    "$Ex1: \\lVert \\beta_i \\rVert_1 =  \\Sigma | \\beta_i |$ , this is called the **L1-norm**\n",
    "\n",
    "$Ex2:  \\lVert \\beta_i \\rVert_2 = \\Sigma \\beta_i^2 $$ $ , this is called the **L2-norm**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These measures of complexity lead to the following regularization techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1 regularization:\n",
    "$$y=Σβ_ix_i + ε \\quad st. \\quad Σ | β_i | \\lt s$$\n",
    "    \n",
    "#### L2 regularization:\n",
    "$$y=Σβ_ix_i + ε \\quad st. \\quad Σ β_i^2 \\lt s$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization refers to the method of preventing overfitting by explicitly controlling model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic purpose of regularisation is to shrink the coefficient estimates which in turn significantly reduce their variance. The two best known techniques are:\n",
    "\n",
    "**Lasso** In Lasso, you are trying to minimise:\n",
    "\n",
    "![](assets/lassoreg.png)\n",
    "\n",
    "**Ridge Regression** In ridge regression, your minimise:\n",
    "\n",
    "![](assets/ridgereg.png)\n",
    "\n",
    "Lambda in both case is called the shrinkage parameter and has to be tuned through cross validation. The second term is called shrinkage penalty.\n",
    "\n",
    "We can think about the use cases of these two more clearly this way:\n",
    "\n",
    "* **L1 regularization**:  Used when we have small data but many features.\n",
    "* **L2 regularization**: Used in just about all other cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clear introduction to the [L-1 and L-2 Norms](http://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/bias_variance_darts.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Bias** refers to predictions that are _systematically_ inaccurate. \n",
    "* **Variance** refers to predictions that are _generally_ inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out (after some math) that the generalization error in our model can be decomposed into a bias component and variance component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](assets/bias_variance_tradeoff.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tradeoff is regulated by a hyperparameter $λ$, which we’ve already seen:\n",
    "\n",
    "### L1 regularization : Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y=Σβ_ix_i + ε \\quad st. \\quad Σ \\lVert β_i \\rVert \\lt λ$$\n",
    "\n",
    "Ridge regression shrinks the parameter estimates but never makes them zero. This might create \"difficult to interpret\" models, especially when there are a large number of variables. Lasso has an inherent property of shrinking some parameter estimates to zero and hence creating more interpretable models while retaining many of the advantages of Ridge regression.  \n",
    "\n",
    "\n",
    "### L2 regularization : Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y=Σβ_ix_i + ε \\quad st. \\quad Σ \\lVert β_i^2 \\rVert \\lt λ$$\n",
    "\n",
    "As you increase lambda, the flexibility of the fit will increase leading to decreased variance but increased bias. When the relationship between the response and the predictors is close to linear, the least square estimate will have low bias but high variance. This will mean that a small change in training data can cause a big change in the parameter estimates. The effect will be even more pronounced when the number the variables is very large. When the number of variables is as large as the number of observations, the least square estimate will be extremely variable.\n",
    "\n",
    "We should take advantage of generalization to trade off variance in our data for bias in our fit, which will overall produce a clearer and better overall fit to our data!"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
