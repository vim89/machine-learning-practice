{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# K Means Clustering Hackathon#\n",
    "\n",
    "*Total Lab Time: 120 Minutes*\n",
    "   \n",
    "*Analysis Time: 105 Minutes*\n",
    "    \n",
    "*Group Discussion Time: 15 Minutes*\n",
    "___\n",
    "\n",
    "K-means clustering is one of the more rudementary and popular unsupervised machine learning algorithms. For this challenge we will use to use KMeans Clustering to cluster Universities into to two groups, Private and Public.\n",
    "\n",
    "*NOTE: For the purposes of the exercise you have been provided the labels **('Private')** which already classifies the universities universities. You will **NOT** use them when fitting your KMeans clustering algorithm, since that would defeat the purpose. That being said; We have provided the labeled dataset so you can compare your model(fitted on the UNLABELED dataset) to the actual labeled dataset*\n",
    "___\n",
    "\n",
    "**1. Please start by using the 'college_data_labeled.csv' dataset and carry out some exploratory data analysis to gather some insights.**\n",
    "\n",
    "**2. Please use the 'college_data.csv' which already contains the dataset without the 'Private' field (or drop the 'Private' field from the 'college_data_labeled.csv') to fit/train your kmeans model.**\n",
    "\n",
    "**3. Once completed, use the entire labeled dataset 'college_data_labeled.csv' to compare your model preditions to corresponding records and compute your model performance metrics.**\n",
    "\n",
    "**4. Once completed, validate your model and determine which K values returns the best results.**\n",
    "\n",
    "___\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "*777 observations on the following 18 variables.*\n",
    "\n",
    "* Apps: Number of applications received\n",
    "* Accept: Number of applications accepted\n",
    "* Enroll Number of new students enrolled\n",
    "* Top10perc: Pct. new students from top 10% of H.S. class\n",
    "* Top25perc: Pct. new students from top 25% of H.S. class\n",
    "* F.Undergrad: Number of fulltime undergraduates\n",
    "* P.Undergrad: Number of parttime undergraduates\n",
    "* Outstate: Out-of-state tuition\n",
    "* Room.Board: Room and board costs\n",
    "* Books: Estimated book costs\n",
    "* Personal: Estimated personal spending\n",
    "* PhD: Pct. of faculty with Ph.D.’s\n",
    "* Terminal: Pct. of faculty with terminal degree\n",
    "* S.F.Ratio: Student/faculty ratio\n",
    "* perc.alumni: Pct. alumni who donate\n",
    "* Expend: Instructional expenditure per student\n",
    "* Grad.Rate: Graduation rate\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "**Import the libraries you usually use for data analysis.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "**You can use either dataset(labeled or unlabeled) provided to conduct you EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means Cluster Algorithm Initialization & Training\n",
    "\n",
    "**Here is the part where YOU WILL HAVE TO DROP THE 'Private' field otherwise you will be including that in the parameters used to fit your model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model to all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing centriod location vs labeled data.\n",
    "\n",
    "**(Skip to the next section if time is of the essence)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "\n",
    "There is no ideal way to evaluate clustering if you don't have the labels, however since this is just an exercise, we do have the labels. You will now take advantage of this to evaluate our clustering model. Note this step is different to model validation which we will do in the next step. Use the documentation links for help.\n",
    "\n",
    "[sklearn: confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "[sklearn: classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation Evaluation\n",
    "\n",
    "**Please refer to the cluster diagnosis lesson to draw inspiration on how to evaluate your model in this next section. We've given your the libraries required to achieve all this in the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read a confusion matrix - Key Terms:\n",
    "\n",
    "[Metrics: Classification Report](http://joshlawman.com/metrics-classification-report-breakdown-precision-recall-f1/)\n",
    "\n",
    "- **true positives (TP)**: These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "- **true negatives (TN)**: We predicted no, and they don't have the disease.\n",
    "- **false positives (FP)**: We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n",
    "- **false negatives (FN)**: We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n",
    "\n",
    "\n",
    "- **Accuracy**: Overall, how often is the classifier correct? (TP+TN)/total = (100+50)/165 = 0.91\n",
    "- **True Positive Rate**: When it's actually yes, how often does it predict yes? TP/actual yes = 100/105 = 0.95 -- also known as **\"Sensitivity\" or \"Recall\"**\n",
    "- **False Positive Rate**: When it's actually no, how often does it predict yes? FP/actual no = 10/60 = 0.17\n",
    "- **True Negative Rate**: When it's actually no, how often does it predict no? TN/actual no = 50/60 = 0.83, equivalent to 1 minus False Positive Rate also known as **\"Specificity\"**\n",
    "- **Precision**: When it predicts yes, how often is it correct? TP/predicted yes = 100/110 = 0.91 \n",
    "- **Prevalence**: How often does the yes condition actually occur in our sample? actual yes/total = 105/165 = 0.64\n",
    "- **Misclassification Rate**: Overall, how often is it wrong? (FP+FN)/total = (10+5)/165 = 0.09 -- (equivalent to 1 minus Accurac), also known as **\"Error Rate\"**\n",
    "\n",
    "### Key Definitions:\n",
    "\n",
    "- **Precision**: Precision is the ability of a classiifer not to label an instance positive that is actually negative. For each class it is defined as as the ratio of true positives to the sum of true and false positives. Said another way, “for all instances classified positive, what percent was correct?”\n",
    "\n",
    "- **Recall**: Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives. Said another way, “for all instances that were actually positive, what percent was classified correctly?”\n",
    "\n",
    "- **F1 score**: The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. Generally speaking, F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n",
    "\n",
    "- **Support**: Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn’t change between models but instead diagnoses the evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
