{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# K Means Clustering Hackathon#\n",
    "\n",
    "*Total Lab Time: 120 Minutes*\n",
    "   \n",
    "*Analysis Time: 105 Minutes*\n",
    "    \n",
    "*Group Discussion Time: 15 Minutes*\n",
    "___\n",
    "\n",
    "K-means clustering is one of the more rudementary and popular unsupervised machine learning algorithms. For this challenge we will use to use KMeans Clustering to cluster Universities into to two groups, Private and Public.\n",
    "\n",
    "*NOTE: For the purposes of the exercise you have been provided the labels **('Private')** which already classifies the universities universities. You will **NOT** use them when fitting your KMeans clustering algorithm, since that would defeat the purpose. That being said; We have provided the labeled dataset so you can compare your model(fitted on the UNLABELED dataset) to the actual labeled dataset*\n",
    "___\n",
    "\n",
    "**1. Please start by using the 'college_data_labeled.csv' dataset and carry out some exploratory data analysis to gather some insights.**\n",
    "\n",
    "**2. Please use the 'college_data.csv' which already contains the dataset without the 'Private' field (or drop the 'Private' field from the 'college_data_labeled.csv') to fit/train your kmeans model.**\n",
    "\n",
    "**3. Once completed, use the entire labeled dataset 'college_data_labeled.csv' to compare your model preditions to corresponding records and compute your model performance metrics.**\n",
    "\n",
    "**4. Once completed, validate your model and determine which K values returns the best results.**\n",
    "\n",
    "___\n",
    "\n",
    "## Data Dictionary\n",
    "\n",
    "*777 observations on the following 18 variables.*\n",
    "\n",
    "* Apps: Number of applications received\n",
    "* Accept: Number of applications accepted\n",
    "* Enroll Number of new students enrolled\n",
    "* Top10perc: Pct. new students from top 10% of H.S. class\n",
    "* Top25perc: Pct. new students from top 25% of H.S. class\n",
    "* F.Undergrad: Number of fulltime undergraduates\n",
    "* P.Undergrad: Number of parttime undergraduates\n",
    "* Outstate: Out-of-state tuition\n",
    "* Room.Board: Room and board costs\n",
    "* Books: Estimated book costs\n",
    "* Personal: Estimated personal spending\n",
    "* PhD: Pct. of faculty with Ph.D.’s\n",
    "* Terminal: Pct. of faculty with terminal degree\n",
    "* S.F.Ratio: Student/faculty ratio\n",
    "* perc.alumni: Pct. alumni who donate\n",
    "* Expend: Instructional expenditure per student\n",
    "* Grad.Rate: Graduation rate\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "\n",
    "**Import the libraries you usually use for data analysis.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in the 'college_data_labeled.csv' file using pd.read_csv(\"<your_file.csv>\",index_col=0) <-- this sets your first column as index.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "**Using the 'college_data_labeled.csv' dataset create a scatterplot of Grad.Rate versus Room.Board where the points are colored by the Private column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a scatterplot of F.Undergrad versus Outstate where the points are colored by the Private column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a stacked histogram showing Out of State Tuition based on the Private column. Try doing this using [sns.FacetGrid](https://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.FacetGrid.html). If that is too tricky, see if you can do it just by using two instances of pandas.plot(kind='hist').**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a similar histogram for the Grad.Rate column.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice how there seems to be a uni with a graduation rate of higher than 100%. That seems strange... find out that name of that school. Set that school's graduation rate to 100 so it makes sense. You may get a warning not an error) when doing this operation, so use dataframe operations or just re-do the histogram visualization to make sure it actually went through.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**recreate the histogram for the Grad.Rate column to ensure that none of the schools don't have a grad rate > 100%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Means Cluster Algorith Initialization & Training\n",
    "\n",
    "**Here is the part where YOU WILL HAVE TO DROP THE 'Private' field otherwise you will be including that in the parameters used to fit your model.**\n",
    "\n",
    "*Import KMeans from SciKit Learn... or just run the cell below since we've already done this to save time for you.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember to set 'inplace=True' and check to ensure it has been dropped before fitting your kmeans model model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create an instance of a K Means model with 2 clusters, you don't have to set the 'random_state=1' but it's generally a good practice. This step IS NOT actually fitting anything to the prepackaged kmeans model, it just creates a model variable for you to call on when fitting your data. We will actually fit our data to our newly instatiated model in the next step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model to all the data.\n",
    "\n",
    "[sklearn: kmeans documentation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(uni_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are '.cluster_centers' & '.labels_'?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing centriod location vs labeled data.\n",
    "\n",
    "**(Skip to the next section if time is of the essence)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Evaluation\n",
    "\n",
    "There is no ideal way to evaluate clustering if you don't have the labels, however since this is just an exercise, we do have the labels. You will now take advantage of this to evaluate our clustering model. Note this step is different to model validation which we will do in the next step. Use the documentation links for help.\n",
    "\n",
    "[sklearn: confusion matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
    "\n",
    "[sklearn: classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a confusion matrix and classification report to see how well the Kmeans clustering worked without being given any labels. We've provided the methods below to help save time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation Evaluation\n",
    "\n",
    "**Please refer to the cluster diagnosis lesson to draw inspiration on how to evaluate your model in this next section. We've given your the libraries required to achieve all this in the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to read a confusion matrix - Key Terms:\n",
    "\n",
    "[Metrics: Classification Report](http://joshlawman.com/metrics-classification-report-breakdown-precision-recall-f1/)\n",
    "\n",
    "- **true positives (TP)**: These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "- **true negatives (TN)**: We predicted no, and they don't have the disease.\n",
    "- **false positives (FP)**: We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n",
    "- **false negatives (FN)**: We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n",
    "\n",
    "\n",
    "- **Accuracy**: Overall, how often is the classifier correct? (TP+TN)/total = (100+50)/165 = 0.91\n",
    "- **True Positive Rate**: When it's actually yes, how often does it predict yes? TP/actual yes = 100/105 = 0.95 -- also known as **\"Sensitivity\" or \"Recall\"**\n",
    "- **False Positive Rate**: When it's actually no, how often does it predict yes? FP/actual no = 10/60 = 0.17\n",
    "- **True Negative Rate**: When it's actually no, how often does it predict no? TN/actual no = 50/60 = 0.83, equivalent to 1 minus False Positive Rate also known as **\"Specificity\"**\n",
    "- **Precision**: When it predicts yes, how often is it correct? TP/predicted yes = 100/110 = 0.91 \n",
    "- **Prevalence**: How often does the yes condition actually occur in our sample? actual yes/total = 105/165 = 0.64\n",
    "- **Misclassification Rate**: Overall, how often is it wrong? (FP+FN)/total = (10+5)/165 = 0.09 -- (equivalent to 1 minus Accurac), also known as **\"Error Rate\"**\n",
    "\n",
    "### Key Definitions:\n",
    "\n",
    "- **Precision**: Precision is the ability of a classiifer not to label an instance positive that is actually negative. For each class it is defined as as the ratio of true positives to the sum of true and false positives. Said another way, “for all instances classified positive, what percent was correct?”\n",
    "\n",
    "- **Recall**: Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives. Said another way, “for all instances that were actually positive, what percent was classified correctly?”\n",
    "\n",
    "- **F1 score**: The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0. Generally speaking, F1 scores are lower than accuracy measures as they embed precision and recall into their computation. As a rule of thumb, the weighted average of F1 should be used to compare classifier models, not global accuracy.\n",
    "\n",
    "- **Support**: Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. Support doesn’t change between models but instead diagnoses the evaluation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
