{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a statistical classification technique based on Bayes Theorem. It is one of the simplest supervised learning algorithms. Naive Bayes classifier is the fast, accurate and reliable algorithm. Naive Bayes classifiers have high accuracy and speed on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features. This assumption is called class conditional independence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions of Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "P(h): the probability of hypothesis h being true (regardless of the data). This is known as the prior probability of h.\n",
    "\n",
    "P(D): the probability of the data (regardless of the hypothesis). This is known as the prior probability.\n",
    "\n",
    "P(h|D): the probability of hypothesis h given the data D. This is known as posterior probability.\n",
    "\n",
    "P(D|h): the probability of data d given that the hypothesis h was true. This is known as posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classifier calculates the probability of an event in the following steps:\n",
    "\n",
    "Step 1: Calculate the prior probability for given class labels\n",
    "\n",
    "Step 2: Find Likelihood probability with each attribute for each class\n",
    "\n",
    "Step 3: Put these values in Bayes Formula and calculate posterior probability.\n",
    "\n",
    "Step 4: See which class has a higher probability, given the input belongs to the higher probability class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:  Probability of Playing Soccer when it is Overcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose a soccer team played outside on 9 total days, and did not play outside for 5 total days. There were 4 overcast days, and they played outside on all 4 days.  There were 5 sunny days and they played outside on 3 of them.  There were 5 rainy days and they played outside on 2 of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes Theorem tells us:\n",
    "\n",
    "P(Yes | Overcast) = P(Overcast | Yes) P(Yes) / P (Overcast) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Prior Probabilities:\n",
    "\n",
    "P(Overcast) = 4/14 = 0.29\n",
    "\n",
    "P(Yes)= 9/14 = 0.64\n",
    "\n",
    "### Calculate Posterior Probabilities:\n",
    "\n",
    "P(Overcast |Yes) = 4/9 = 0.44\n",
    "\n",
    "### Put Prior and Posterior probabilities in equation (1)\n",
    "\n",
    "P (Yes | Overcast) = 0.44 * 0.64 / 0.29 = 0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import scikit-learn dataset library\n",
    "from sklearn import datasets\n",
    "\n",
    "#Load dataset\n",
    "wine = datasets.load_wine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "Labels:  ['class_0' 'class_1' 'class_2']\n"
     ]
    }
   ],
   "source": [
    "# print the names of the 13 features\n",
    "print(\"Features: \", wine.feature_names)\n",
    "\n",
    "# print the label type of wine(class_0, class_1, class_2)\n",
    "print(\"Labels: \", wine.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rcrouch/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Import train_test_split function\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3,random_state=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Gaussian Naive Bayes model\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "#Train the model using the training sets\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.907407407407\n"
     ]
    }
   ],
   "source": [
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "from sklearn import metrics\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "It is not only a simple approach but also a fast and accurate method for prediction.\n",
    "\n",
    "Naive Bayes has very low computation cost.\n",
    "\n",
    "It can efficiently work on a large dataset.\n",
    "\n",
    "It performs well in case of discrete response variable compared to the continuous variable.\n",
    "\n",
    "It can be used with multiple class prediction problems.\n",
    "\n",
    "It also performs well in the case of text analytics problems.\n",
    "\n",
    "When the assumption of independence holds, a Naive Bayes classifier performs better compared to other models like logistic regression.\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "The assumption of independent features. In practice, it is almost impossible that model will get a set of predictors which are entirely independent.\n",
    "\n",
    "If there is no training tuple of a particular class, this causes zero posterior probability. In this case, the model is unable to make predictions. This problem is known as Zero Probability/Frequency Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
